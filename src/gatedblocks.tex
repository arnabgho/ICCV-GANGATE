\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{amsbsy}
\usepackage{array, caption, floatrow, tabularx, makecell, booktabs}%
\usepackage{animate}
\usepackage{floatrow}
\usepackage{etoolbox}


\setlength{\belowcaptionskip}{-1em}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{4121} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\usepackage{xcolor}
\usepackage[export]{adjustbox}

%%%%%%%%%%%%%%
% Packages and stuff custom
%%%%%%%%%%%%%%
\usepackage{url}
\usepackage{amsthm}
\usepackage{color}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{booktabs}
\usepackage{arydshln}
%\usepackage{hyperref}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\R}{\mathbb{R}}

\def\figref#1{Fig.~\ref{#1}}
\def\secref#1{\S\ref{#1}}
\def\tabref#1{Table~\ref{#1}}
\def\eqnref#1{Eq.~\ref{#1}}

\newcommand{\ow}[1]{\textbf{\textcolor[rgb]{.1, .1, .8}{OW: #1}}}
\newcommand{\rz}[1]{\textbf{\textcolor[rgb]{.54, .16, .55}{RZ: #1}}}
\newcommand{\todo}[1]{\textbf{\textcolor[rgb]{.8, .1, .1}{#1}}}
\newcommand{\pd}[1]{\textbf{\textcolor[rgb]{1, 0.5, 0}{PD: #1}}}
\newcommand{\es}[1]{\textbf{\textcolor[rgb]{0.1, 0.4, 0.1}{ES: #1}}}
\newcommand{\ag}[1]{\textbf{\textcolor[rgb]{0.45, 0.25, 0.1}{AG: #1}}}
\newcommand{\cameraready}[1]{\textcolor[rgb]{0.5, 0.25, 0.15}{#1}}
%% spacehacks
%\setlength{\abovecaptionskip}{-5pt}


\newcommand{\addSubFigHalf}[3]{\begin{subfigure}[t]{.45\linewidth}
   \includegraphics[width=\linewidth]{#1}
   \caption{#2}\label{#3}\end{subfigure}
}
\newcommand{\addSubFigThird}[3]{\begin{subfigure}[t]{.31\linewidth}
   \includegraphics[width=\linewidth]{#1}
   \caption{#2}\label{#3}\end{subfigure}
}
\newcommand{\addSubFigTenth}[3]{\begin{subfigure}[t]{.16\linewidth}
   \includegraphics[width=\linewidth]{#1}
   \caption{#2}\label{#3}\end{subfigure}
}
\newcommand{\addSubFigSixth}[2]{\begin{subfigure}[t]{.12\linewidth}
   \includegraphics[width=\linewidth]{#1}
   \label{#2}\end{subfigure}
}
\newcommand{\addSubFigSixthLabel}[3]{\begin{subfigure}[t]{.12\linewidth}
   %\rotatebox{90}{#2}
   \includegraphics[width=.9\linewidth]{#1}
   \caption{#2}\label{#3}\end{subfigure}
}



\newcommand{\model}[0]{GateGAN}

\DeclareGraphicsExtensions{.pdf,.jpg}

%%%%%%%%%%%%%%%

\graphicspath{ {images/}{syntheticExp/} {final_images/channel_gated/} {final_images/} {paper_images/} }
%%%%%%%%%%%%%%%%


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{4121} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Interactive Sketch \& Fill: Multiclass Sketch-to-Image Translation}
%\title{Interactive Sketch and Fill: Multiclass Outline-to-Image with Gated Residual Networks}
%\title{Complete and Translate: Multiclass Outline-to-Image with Gated Residual Networks}
% \title{GAN-Gate: Gated Residual GANs for Class-Conditioned Image Generation}
% GAN-Gate: Conditionally-Gated Residual Networks for Image-to-Image Translation
% GAN-Gate: Softly-Gated Conditional Residual Networks for Image-to-Image Translation
% GAN-Gate: Conditionally Soft-Gated Residual Networks for Image-to-Image Translation
%GAN-Gate: Conditional Gating for Image-to-Image Translation

% \author{Arnab Ghosh\\
% University of Oxford\\
% {\tt\small arnab.ghosh@eng.ox.ac.uk}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Richard Zhang\\
% Adobe Research\\
% {\tt\small rizhang@adobe.com}
% \and
% Puneet Dokania\\
% University of Oxford\\
% {\tt\small puneet@robots.ox.ac.uk}
% \and
% Oliver Wang\\
% Adobe Research\\
% {\tt\small owang@adobe.com}
% \and
% Alexei A. Efros\\
% UC Berkeley\\
% {\tt\small efros@eecs.berkeley.edu}
% \and
% Philip Torr\\
% University of Oxford\\
% {\tt\small philip.torr@eng.ox.ac.uk}
% \and
% Eli Shechtman\\
% Adobe Research\\
% {\tt\small elishe@adobe.com}
% }

% \author{
% Arnab Ghosh$^1$, Richard Zhang$^2$ , Puneet K. Dokania$^1$ ,\\
% Oliver Wang$^2$,  Alexei A. Efros$^2^,^3$, Philip H.S. Torr$^1$, Eli Shechtman $^2$ \vspace{5pt} \\
% $^1$University of Oxford \quad $^2$Adobe Research \quad $^3$ UC Berkeley \vspace{5pt} \\ 
% {\tt \{arnabg,puneet,phst\}@robots.ox.ac.uk,\{rizhang,owang,elishe\}@adobe.com}, \\ {\tt efros@eecs.berkeley.edu}
% }

\author{
Arnab Ghosh$^{1}$ \hspace{8mm} Richard Zhang$^{2}$ \hspace{8mm} Puneet K. Dokania$^{1}$ \\
Oliver Wang$^{2}$ \hspace{8mm} Alexei A. Efros$^{2,3}$ \hspace{8mm} Philip H. S. Torr$^{1}$ \hspace{8mm} Eli Shechtman$^{2}$ \\
\\
$^{1}$University of Oxford \hspace{15mm} $^{2}$Adobe Research \hspace{15mm} $^{3}$UC Berkeley \\
% Arnab Ghosh$^{1}$ \hspace{6mm} Richard Zhang$^{2}$ \hspace{6mm} Puneet K. Dokania$^{1}$ \hspace{6mm} Oliver Wang$^{2}$\\
% Alexei A. Efros$^{2,3}$ \hspace{6mm} Philip H. S. Torr$^{1}$ \hspace{6mm} Eli Shechtman$^{2}$ \\
% \\
% $^{1}$University of Oxford \hspace{2.3cm} $^{2}$Adobe Research \hspace{2.3cm} $^{3}$UC Berkeley \\
% {\tt\footnotesize \{arnabg,puneet,phst\}@robots.ox.ac.uk \hspace{2mm} \{\tt\footnotesize rizhang,owang,elishe\}@adobe.com} \hspace{2mm} {\tt\footnotesize \{efros\}@eecs.berkeley.edu}
% \vspace*{-2mm}
}

% \author{Richard Zhang$^{1}$ \hspace{3mm} Phillip Isola$^{12}$ \hspace{3mm} Alexei A. Efros$^{1}$\\
% $^{1}$UC Berkeley \hspace{3mm} $^{2}$OpenAI\\
% {\tt\small \{rich.zhang, isola, efros\}@eecs.berkeley.edu}
% \and
% Eli Shechtman$^{3}$ \hspace{3mm} Oliver Wang$^{3}$\\
% $^{3}$Adobe Research\\
% {\tt\small \{elishe,owang\}@adobe.com}
% }

%\maketitle
%\thispagestyle{empty}


\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
    \centering
    % \includegraphics[width=.9\linewidth,height=4.5cm]{images/teaser/sketch.jpg}
    % \includegraphics[width=1.\linewidth]{paper_images/teaser_autocomplete_v4.pdf}
    % \vspace{-20mm}
    \includegraphics[width=1.\linewidth]{paper_images/teaser_v7.pdf}
    
    \captionof{figure}{({\bf Top}) Given sparse user input (first row), our model estimates the complete shape and provides this as a recommendation to the user (shown in gray), along with the final synthesized object (second row). These estimates are updated as the user adds (\textcolor{green}{green}) or removes strokes (\textcolor{red}{red}) over time -- previous edits are shown in black.
    ({\bf Bottom}) This generation is class-conditioned, and our method is able to generate distinct multiple objects for the same outline (\eg `circle') by conditioning the generator on the object category.\label{fig:teaser}
    }
    \vspace{1em}
\end{center}%
}]
    
%%%%%%%%% ABSTRACT
\input{src/0_abstract.tex}

\input{src/1_introduction.tex}

\input{src/2_relatedwork.tex}

% \input{src/3_preliminary.tex}

\input{src/3_methods.tex}

\input{src/4_experiments.tex}

\input{src/5_discussion.tex}
\vspace{-4mm}
\section*{Acknowledgements}
\noindent AG, PKD, and PHST are supported by the ERC grant ERC-2012-AdG, EPSRC grant Seebibyte EP/M013774/1, EPSRC/MURI grant EP/N019474/1 and would also like to acknowledge the Royal Academy of Engineering and FiveAI. Part of the work was done while AG was an intern at Adobe.


%\section{Acknowledgements}
%AG, PKD, and PHST are supported by the ERC grant ERC-2012-AdG, EPSRC grant Seebibyte EP/M013774/1, EPSRC/MURI grant EP/N019474/1 and would also like to acknowledge the Royal Academy of Engineering and FiveAI.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{src/gatedblocks}
}

% \input{src/supplemental.tex}

\end{document}


